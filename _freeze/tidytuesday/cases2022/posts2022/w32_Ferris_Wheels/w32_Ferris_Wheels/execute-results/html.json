{
  "hash": "57ed3d89904fb6b32ae1cb7e14fe9bb4",
  "result": {
    "markdown": "---\ntitle: 'Ferris Wheels'\nsubtitle: 'Welcome to #TidyTuesday 2022 day 32'\ndate: '2022-08-09'\nimage: 'https://raw.githubusercontent.com/Fgazzelloni/TidyTuesday/main/data/2022/w32_ferriswheels/w32_ferriswheels.png'\nimage-alt: ''\ndescription: 'Networks'\noutput: html_document\nexecute: \n   eval: false\n---\n\n\n\n\nData this #TidyTuesday 2022 week32 is from [EmilHvitfeldt](https://github.com/EmilHvitfeldt/ferriswheels/blob/main/data-raw/wheels.R).\n\n\n::: {.cell paged.print='false'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nwheels <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-08-09/wheels.csv')\n```\n:::\n\n\nThis dataset contains information about ferris wheels around the world. The objective of this visualization is to provide a visualization of the differences among ferris wheels in terms of diameter and number of cabins. There is a 48% and 15% missing values in the diameter, and number of cabins vectors, respectively. So, what we need to do is find a way to impute these missing values. There are several techniques used for imputing missing values, such as trees, KNN, SVM, mean/median, linear, and polynomial imputations. We could use {tidymodels} and provided `step_functions()` in order to do that, passing through the use of the `recipe()` function, or workout a custom way to imputation. This is when you look at the data and impute missing values based on the overall meaning. I'll call it *custom sintetic imputation*.\n\n\n::: {.cell}\n\n:::\n\n\nBefore to start tidying our dataset, we exclude the *Nippon* wheel as it is the only one without height and it is still on development, searching on the internet, it is difficult to find information about it, so we exclude it. The, a second ferris wheel which requires some attention is the *Big O*, it has a height which is lower than its diameter, and if you Google it: [Big O](https://www.google.com/search?q=height+big+o+ferris+wheel&rlz=1C5CHFA_enIT984IT984&sxsrf=ALiCzsYPO2d-xLz_ncWVp28RsK6Z1tuHlQ%3A1660635889905&ei=8Ur7YtLcNsO6xc8P7fezoAo&ved=0ahUKEwiSmcbi7sr5AhVDXfEDHe37DKQQ4dUDCA4&uact=5&oq=height+big+o+ferris+wheel&gs_lcp=Cgdnd3Mtd2l6EAM6BwgAEEcQsAM6DQguEMcBEK8BELADEEM6BwgAELADEEM6BwgjELACECc6CAgAEB4QDRATSgQIQRgASgQIRhgAUJkEWIAUYIoaaAFwAXgAgAHcAYgBpAeSAQUwLjYuMZgBAKABAcgBCsABAQ&sclient=gws-wiz) you'll find that both height and diameter are 60 mt (197 feet), so I decided to swap the values, considering the higher value as the height and the other as the diameter.\n\n\n[Moscow-850](https://en.wikipedia.org/wiki/Moscow-850) is expressed in mt so it needs to be in ft, about 240 ft.\n[Chicago Wheel](https://en.wikipedia.org/wiki/Ferris_Wheel_(1893)) is in mt as well, so considering it in ft, it would be \n\n\n::: {.cell}\n\n```{.r .cell-code}\nwheels1 <- wheels%>% \n  filter(!str_detect(name,\"Nippon\"))%>% # excluded\n  mutate(location=ifelse(is.na(location),\"Dubai\",location),\n         diameter=case_when(name==\"Moscow-850\"~240,\n                            name==\"Chicago Wheel\"~height,\n                            TRUE~diameter),\n         temp=ifelse(name==\"Big O\",height,diameter),\n         height=ifelse(name==\"Big O\",diameter,height),\n         diameter=ifelse(name==\"Big O\",temp,diameter)) \nwheels1%>%\n  select(name,country,height,diameter)%>%\n  filter(name%in%c(\"Big O\",\"Moscow-850\",\"Chicago Wheel\"))\n```\n:::\n\nThen, another cleaning step involves the `cost` and `tickets` variables (renamed from `construction_cost` and `ticket_cost_to_ride`), these two vectors are important to be just numeric, because we are going to use them inside a model to predict suitable missing values for the `number of cabins`; in addition, when multiple values are provided for tickets, such as adult and children, we include just the *adults* ticket prices.\nIn order to do that, we make some text customization, named regex, text regular expressions with the help of `stringr::str_extract()` function, as shown below. We have some missing values among those two vectors, 55% and 43% for costs and tickets, respectively. Also, these two vector will be imputed to be used in our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwheels1%>% \n  mutate(costs=stringr::str_extract(construction_cost, \"\\\\d+\\\\.*\\\\d*\"),\n         costs=as.numeric(costs),\n         tickets=stringr::str_extract(ticket_cost_to_ride, \"\\\\d+\\\\.*\\\\d*\"),\n         tickets=as.numeric(tickets)) %>%\n  filter(!is.na(costs))%>%\n  select(name, country,costs,tickets)%>%head\n```\n:::\n\nAs said, we suppose that height is always higher than diameter. A Ferris wheel is generally, lifted on a wheel support and the height is the height of the wheel plus its support. So that, the diameter is just the diameter of the wheel in itself. We do not have missing values on the height vector, the only missing value (Nippon wheel) has been excluded. To replace the missing values in the diameter vector, we have a look at the relationship between diameter versus height. It is about linear. \n\n\n::: {.cell}\n\n:::\n\nMore considerations are about the ferris wheel *support* height, the height of the rim steel structure of the ferris wheel, this is given by the difference between the overall height and radius of the ferris wheel. Here is considered just the distance from the end of the wheel and the ground base of the support. We consider the median difference of the height minus the diameter of the all ferris wheel in the dataset, and build a **sintetic_diameter** vector filling missing values with the difference between height and the median difference of 33ft.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmed_bs_hg=median(height-diameter,na.rm = T)\nsintetic_diameter=ifelse(is.na(diameter),height-med_bs_hg,diameter)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nTo adopt the median difference is quite superficial, but for now consider that as a base structure for all ferris wheel to start with. 33 ft will be the median distance of the ferris wheels from the ground, in our model.\n\nThen, the *number of cabins* vector contains 15% of missing values, and here these values are covered with some extra manipulations.\n\nConsider some geometry, how to calculate the circumference, the [arc length](https://www.cuemath.com/geometry/arc-length/), or the distance among cabins, and the angles such as *theta*. This apparently a redundant procedure, can lead to reduced bias, as theta is of determined range from 0 to 360 degree or $2\\pi$ radiant. The number of cabins can also be of a certain range, as not more than a well specified number of cabins are allowed depending on diameter of the ferris wheel, but this is unknown and potentially be of any unspecified length. For this reason $\\theta$ could be a good estimator.\n\nThe *number of cabins* are points on a circle, and for those values in the dataset which are not missing, an arc length can be calculated, as long as the central angle $\\theta$. Once these values are set, the missing values can be calculated with a backwards procedure.\n\n$$L=\\text{Length of an arc}$$\n$$r=\\text{Radius}$$\n$$L=\\theta\\frac{\\pi}{180}r$$ \nA **sintetic number of cabins** (`sintetic_n_cab`) vector is set to be filled with imputed missing values. To find these values, some other parameters are needed. The `sintetic_diameter` vector can be used to find the *circumference*, as well as the *radius*, the arc length among points (the `arc_distance`) and finally $\\theta$. All these values are now filled with missing values, as are derived from the *number of cabins vector*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncircumference = pi*sintetic_diameter\narc_distance = circumference/number_of_cabins\ntheta = (arc_distance*180)/(pi*sintetic_diameter/2)\nsintetic_n_cab = circumference/arc_distance\n```\n:::\n\n\nFinal data manipulation below shows how new features are made ready to be used in the model. For further reference [Feature Engineering and Selection](http://www.feat.engineering/engineering-numeric-predictors.html) - Engineering Numeric Predictors, pg.122 is about expanding individual predictors into many predictors.\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_df <- wheels1%>%\n  select(-temp,-`...1`) %>%\n  mutate(med_bs_hg=median(height-diameter,na.rm = T),\n         sintetic_diameter=ifelse(is.na(diameter),height-med_bs_hg,diameter),\n         #sintetic_height=sintetic_diameter+med_bs_hg,\n         circumference=pi*sintetic_diameter,\n         arc_distance=circumference/number_of_cabins,\n         theta=(arc_distance*180)/(pi*sintetic_diameter/2),\n         sintetic_n_cab=circumference/arc_distance)%>%\n  \n  select(-construction_cost,-ticket_cost_to_ride)%>%\n  arrange(country)\n```\n:::\n\n\nSo, we are left with some missing values in the number of cabins. Before going to modeling, some missing values can be filled considering the mean of the arc_distance for the group of heights, this values will be used to fill some of the missing number of cabins for those values with a common height. \n\n::: {.cell}\n\n:::\n\n\nLooking at the distribution of $\\theta$, it is clear a right skewness of the distribution. As said, the angle $\\theta$, which is between (0,360), might be a good estimator to use for estimating missing number of cabins, passing through the arch length, the distance among cabins on the wheel.\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_df1%>%\n  ggplot(aes(theta))+\n  geom_histogram(bins=10,color=\"white\")+\n  ggthemes::theme_fivethirtyeight()+\n  labs(title=expression(paste(\"Distribution:\\t\",theta)))\n```\n:::\n\n\n\nA trees based model would be the best choice for guessing these values (ct. [Feature Engineering and Selection](http://www.feat.engineering/engineering-numeric-predictors.html) - Engineering Numeric Predictors, pg.121). \n\nOne more consideration is due to the dimension of the dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\nwheels%>%dim\n```\n:::\n\n\nCross validation could be a solution, as it shuffles data on specified number of folds. In this case the `group_vfold_cv()` function is used to make the cross validation folds grouped by height while predicting theta.\n\nMore about how to extrapolate vfold_cv() assessment/analysis datasets here: [article](https://www.tidyverse.org/blog/2022/08/rsample-1-1-0/)\n\n\n::: {.cell paged.print='false'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\n\nset.seed(1234)\nfolds <- group_vfold_cv(my_df1, group = height,v = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- my_df1%>%\n  recipe(theta~circumference+height+sintetic_diameter+sintetic_n_cab) %>%\n  step_corr(all_numeric()) %>% \n  step_impute_bag(theta,sintetic_n_cab)\n\nrec%>%prep()%>%bake(new_data=NULL)%>%DataExplorer::profile_missing()\n```\n:::\n\nSet a Random forest engine *randomForest* with tuning parameters. **mtry** is for feature subset strategy, in this case just three predictors are used. **min_n** is the min node size, and then there is the number of **trees**. \n\n::: {.cell}\n\n```{.r .cell-code}\nshow_engines(\"rand_forest\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_model_info(\"rand_forest\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <-\n  rand_forest(mtry = tune(), \n              trees = tune(),\n              min_n = tune()) %>%\n  set_engine('randomForest') %>%\n  set_mode('regression')\n\nrf_wfl <- workflow() %>%\n  add_model(rf_spec) %>%\n  add_recipe(rec)\n\ndoParallel::registerDoParallel()\nset.seed(123)\nrf_res <- tune_grid(object = rf_wfl,\n                    resamples = folds,\n                    grid = 20,\n                    control = control_grid(save_pred = T,\n                                           save_workflow = T,\n                                           verbose = T,\n                                           parallel_over = \"everything\"))\n\nrf_res %>% \n  collect_metrics()%>%\n  filter(.metric==\"rmse\")%>%\n  arrange(mean)%>%\n  select(mtry,trees,min_n,mean)%>%\n  pivot_longer(cols=mtry:min_n,names_to=\"parameters\",values_to=\"values\")%>%\n  ggplot(aes(values,mean, color=parameters))+\n  geom_point(show.legend = F)+\n  facet_wrap(~parameters,scale=\"free\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_grid <- grid_regular(\n  trees(range = c(500,1000)),\n  min_n(range = c(0,5)),\n  mtry(range=c(2,2)),\n  levels = 5\n)\nrf_grid\n```\n:::\n\n\nSecond tuning with a specified range of parameters.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(456)\nrf_res2 <- tune_grid(object = rf_wfl,\n                    resamples = folds,\n                    grid = rf_grid,\n                    control = control_grid(save_pred = T,\n                                           save_workflow = T,\n                                           verbose = T,\n                                           parallel_over = \"everything\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res2 %>%\n  collect_metrics()%>%\n  filter(.metric==\"rmse\")%>%\n  arrange(mean)%>%\n  select(mtry,trees,min_n,mean) %>%\n  mutate(min_n=as.factor(min_n))%>%\n  ggplot(aes(trees,mean, color=min_n))+\n  geom_line()+\n  geom_point(show.legend = F)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(rf_res2,\"rsq\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_df2<-my_df1%>%\n  filter(!is.na(theta))\n\ntest<- my_df1%>%\n  filter(is.na(theta))\n\n\npred_theta <- workflow()%>%\n  add_model(rf_spec)%>%\n  add_formula(formula=theta~sintetic_diameter)%>%\n  finalize_workflow(select_best(rf_res2)) %>%\n  fit(data = my_df2) %>%\n  augment(new_data=test) %>%\n  select(sintetic_n_cab,arc_distance,theta,.pred)\n\npred_theta  \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n  my_df1$theta[is.na(my_df1$theta)]  <-  pred_theta$.pred\n  \n  my_imputed_df <- my_df1%>%\n    mutate(sintetic_distance=ifelse(is.na(arc_distance),\n                                    (theta*pi*sintetic_diameter)/360,arc_distance),\n           sintetic_n_cab=ifelse(is.na(number_of_cabins),\n                                 (circumference/sintetic_distance),sintetic_n_cab))%>%\n    select(name,country,height,sintetic_diameter,diameter,sintetic_n_cab)\n  \n  my_imputed_df%>%head\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_imputed_df1 <- my_imputed_df %>%\n  mutate(y = height-sintetic_diameter/2,\n         r = sintetic_diameter / 2) %>%\n  group_by(country) %>%\n  mutate(country_id=1,\n         name=paste0(name,\"\\n\",country),\n         tot_wheels=sum(country_id),.after=country)%>%\n  ungroup() %>%\n  mutate(country=as.character(country)) %>%\n   mutate(name=case_when(name==\"Diamond and Flower Ferris Wheel\\nJapan\"~\"Diamond and Flower\\nFerris Wheel\\nJapan\",TRUE~name))\nmy_imputed_df1%>%head\n```\n:::\n\n\nThis code is from [EmilHvitfeldt](https://github.com/EmilHvitfeldt/ferriswheels/blob/main/data-raw/wheels.R), who has provided the data for this #TidyTuesday 2022 week32.\n\n::: {.cell}\n\n```{.r .cell-code}\n cabin <- my_imputed_df1 %>% \n    group_by(name) %>% # summarise(number_of_cabins)\n    summarise(cabin = seq_len(sintetic_n_cab),\n              # Get x and y for the carts\n              cabin_x = cos(cabin / sintetic_n_cab * 2 * pi),\n              cabin_y = sin(cabin / sintetic_n_cab * 2 * pi),\n              # Size them to be the right distance from the center\n              cabin_x = cabin_x * (sintetic_diameter / 2),\n              cabin_y = cabin_y * (sintetic_diameter / 2),\n              # Make sure the carts are raised enough\n              cabin_y = cabin_y + height - sintetic_diameter / 2,\n              # Lower the carts just a bit so it appears they are hanging\n              cabin_y = cabin_y - 12.5,\n              cabin_color = as.character(cabin %% 3),\n              sintetic_diameter,sintetic_n_cab,\n              .groups = \"drop\"\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_imputed_df1 %>%\n # filter(str_detect(name,\"Flower\"))%>%select(name)\nggplot() +\n    geom_abline(slope = 0, intercept = 0, color = \"darkgreen\") +\n    ylim(0, NA) +\n    ggforce::geom_circle(aes(x0 = 0, y0 = y, r = r),\n                         color=\"midnightblue\") +\n    # # Left leg\n    geom_segment(aes(x = -(height - sintetic_diameter / 2)/2, \n                     xend = 0,\n                     yend = height - sintetic_diameter / 2, \n                     y = 0)) +\n    # right leg\n    geom_segment(aes(x = (height - sintetic_diameter / 2)/2, \n                     xend = 0,\n                     yend = height - sintetic_diameter / 2, \n                     y = 0)) +\n    geom_point(data = cabin,\n               mapping=aes(cabin_x, cabin_y, \n                           color = cabin_color,\n                           fill = cabin_color),\n               size=0.01,\n               shape = 24) +\n    labs(title=\"Ferris Wheels: overview by dimensions\",\n         subtitle = \"Diameters and number of cabins are imputed. Ferris wheels are ordered by number of cabins.\\nOn average: distance from base ground is 33ft, sintetic diameters are 271ft and number of cabins are 42.\\n\",\n         caption = \"Data are from #TidyTuesday 2022 week 32 {ferriswheels} by @Emil_Hvitfeldt\\nDataViz: Federica Gazzelloni @fgazzelloni\")+\n    theme_void()+\n    theme(text = element_text(family=\"Roboto Condensed\", size=14),\n          plot.title = element_text(size=45),\n          plot.subtitle = element_text(size=20),\n          plot.caption = element_text(size=20,hjust = 0.5,vjust=0.5),\n          plot.background = element_rect(fill=\"gray80\",color=\"gray80\"),\n          panel.background = element_rect(fill=\"gray80\",color=\"gray80\"),\n          legend.position = \"none\")+\n    facet_wrap(~fct_reorder(name,sintetic_n_cab))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n  ggsave(\"w32_ferriswheels.png\",\n         dpi=320,\n         width = 15,\n         height = 18)\n```\n:::\n\n\nJust a little check for some of the ferris wheels with out of the mean overall height.\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_imputed_df1 %>%filter(str_detect(name,\"Roue|HEP\"))%>%select(name,diameter,height,sintetic_diameter)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_imputed_df1%>%select(sintetic_diameter,sintetic_n_cab)%>%map_dfr(mean)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}